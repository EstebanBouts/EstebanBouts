{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZN3JWyxKrhI"
      },
      "source": [
        "# Importation des Librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "GiEVxyFpKrhJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KAJdplTKrhK"
      },
      "source": [
        "## 1. Préparation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJnygPsVKrhK"
      },
      "source": [
        "1.1 Importer les données Train et Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Mb3dMI3vLySe",
        "outputId": "c6aa64d1-2eda-4b65-f8b4-ac0fa6ff0f8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_uuid": "f66eed17ca77cd4973b8f00c510c5d98e4dc2af7",
        "id": "KFUl3OpFKrhK"
      },
      "outputs": [],
      "source": [
        "train_path = '/content/drive/MyDrive/Data_Science/train_clean.csv'\n",
        "test_path = '/content/drive/My Drive/Data_Science/test_clean.csv'\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7AA1v9eKrhL"
      },
      "source": [
        "1.2 Depuis le Dataframe train, charger les features d'apprentissage dans un array numpy X_alltrain, et les labels (données à prévoir) dans un array numpy y_alltrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TEZG97HGKrhL"
      },
      "outputs": [],
      "source": [
        "X_alltrain = train_df.drop(columns=['Survived']).to_numpy()\n",
        "y_alltrain = train_df['Survived'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXayweMHKrhL"
      },
      "source": [
        "1.3 Séparer les features et les labels en deux parties (train et dev), en attribuant 10% des exemples aux données de dev. afficher les nombres de lignes et de colonnes pour les 4 arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b",
        "id": "fYsLNOzIKrhL",
        "outputId": "8adec7d6-5c78-48d0-a9d8-4387ef6006c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (801, 8)\n",
            "y_train shape: (801,)\n",
            "X_dev shape: (90, 8)\n",
            "y_dev shape: (90,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_dev, y_train, y_dev = train_test_split(X_alltrain, y_alltrain, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_dev shape: {X_dev.shape}\")\n",
        "print(f\"y_dev shape: {y_dev.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDZWoZT2KrhL"
      },
      "source": [
        "1.4 Afficher les 10 premières lignes de features et les 10 premiers labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b",
        "id": "v_BvnlgGKrhM",
        "outputId": "a198e244-02e7-481f-c1c3-caacba8290cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 premieres lignes des features:\n",
            "[[166   3   1   0   2   0   0   4]\n",
            " [542   3   0   0   3   0   0   2]\n",
            " [626   1   1   3   3   0   1   1]\n",
            " [389   3   1   2   0   2   1   1]\n",
            " [ 77   3   1   0   0   0   1   1]\n",
            " [282   3   1   1   0   0   1   1]\n",
            " [713   1   1   2   3   0   0   1]\n",
            " [339   3   1   2   1   0   1   1]\n",
            " [328   2   0   2   1   0   1   3]\n",
            " [322   3   1   1   0   0   1   1]]\n",
            "10 premiers labels:\n",
            "[1 0 0 0 0 0 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"10 premieres lignes des features:\")\n",
        "print(X_train[:10])\n",
        "print(\"10 premiers labels:\")\n",
        "print(y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Jc2mn7KrhM"
      },
      "source": [
        "## 2. Définition du réseau de neurone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMqbAEIzKrhM"
      },
      "source": [
        "2.1 Définir et instancier une classe Titanic Model avec les caractéristiques suivantes :\n",
        "- deux  couches cachées de 50 neurones.\n",
        "- deux classes en sortie : Survivant ou non\n",
        "- Des fonctions d'activation RELU\n",
        "- un dropout paramétrable pour les 2 couches cachées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zm2M7aMFKrhM"
      },
      "outputs": [],
      "source": [
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self, input_size, dropout_prob):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_size, 50)\n",
        "        self.hidden2 = nn.Linear(50, 50)\n",
        "        self.output = nn.Linear(50, 2)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.hidden2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "dropout_prob = 0.5\n",
        "model = TitanicModel(input_size, dropout_prob)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3n-Q8KLKrhM"
      },
      "source": [
        "2.2 : Définir des paramètres de nombre d'epochs (50) et de learning_rate (0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4",
        "id": "ay5wHpznKrhN"
      },
      "outputs": [],
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwuaSihvKrhN"
      },
      "source": [
        "2.3 définir la taille du minibatch à 50. En déduire le nombre de boucle pour chaque epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4",
        "id": "J3ndMC4jKrhN",
        "outputId": "30668b68-399d-4a5f-d349-ea192d0d2985",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de boucles par epoch: 16\n"
          ]
        }
      ],
      "source": [
        "batch_size = 50\n",
        "n_batches = len(X_train) // batch_size\n",
        "print(f\"Nombre de boucles par epoch: {n_batches}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOnshl1RKrhN"
      },
      "source": [
        "2.4 Définir un fonction de coût de type CrossEntropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102",
        "id": "3H2_uRt6KrhN"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4_WlvJUKrhN"
      },
      "source": [
        "2.5. Définir un optimizer de type Adam, sans oublier le learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102",
        "id": "YYKTRR_fKrhO"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVXCfeO3KrhO"
      },
      "source": [
        "## 3. Apprentissage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1Zm5DrKrhO"
      },
      "source": [
        "3.1 exécuter l'apprentissage du modèle.\n",
        "- Créer une boucle sur les epochs, qui contient elel-même une boucle sur les minibatchs.\n",
        "- A chaque nouvelle itération sur les epochs, mélanger les données avec la méthode shuffle.\n",
        "- Tous les 5 epochs afficher la valeur de la fonction de cout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "_uuid": "9f2d8485cc5aac6b03a1aca4c163b5c6f5cd4259",
        "id": "fWqamc9aKrhO",
        "outputId": "80a1b10a-f107-41f4-86de-d246b1187d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Perte moyenne: 0.6519\n",
            "Epoch 10/50, Perte moyenne: 0.6539\n",
            "Epoch 15/50, Perte moyenne: 0.6483\n",
            "Epoch 20/50, Perte moyenne: 0.6729\n",
            "Epoch 25/50, Perte moyenne: 0.6827\n",
            "Epoch 30/50, Perte moyenne: 0.6507\n",
            "Epoch 35/50, Perte moyenne: 0.6505\n",
            "Epoch 40/50, Perte moyenne: 0.6739\n",
            "Epoch 45/50, Perte moyenne: 0.6509\n",
            "Epoch 50/50, Perte moyenne: 0.6727\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Perte moyenne: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a4thg04KrhO"
      },
      "source": [
        "3.2 Calculer la précision de la prévision sur les données dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "_uuid": "b30b4c578b3ddd40ec9f63e19396b045c5ac0dd5",
        "id": "LlpA1B7gKrhO",
        "outputId": "8cb01c00-4282-465a-8cd1-61b70cd1d191",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Précision : 0.5889\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_dev_tensor = torch.tensor(X_dev, dtype=torch.float32)\n",
        "y_dev_tensor = torch.tensor(y_dev, dtype=torch.long)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits = model(X_dev_tensor)\n",
        "  predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "accuracy = accuracy_score(y_dev, predictions.numpy())\n",
        "print(f\"Précision : {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMuLUA7iKrhO"
      },
      "source": [
        "3.3 Calculer prévisions sur les données de tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "_uuid": "10054399a4160803528137e2ebf60b73394b6007",
        "id": "YTCo0CMGKrhO"
      },
      "outputs": [],
      "source": [
        "X_test_tensor = torch.tensor(test_df.values, dtype=torch.float32)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits_test = model(X_test_tensor)\n",
        "  predictions_test = torch.argmax(logits_test, dim=1)\n",
        "predictions_test = predictions_test.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jEoaJ3NKrhO"
      },
      "source": [
        "3.4 Générer le fichier résultat et l'envoyer sur kaggle\n",
        "Quel est votre score et votre classement ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "_uuid": "901d7d529bfe936d8c5091e2a73ffc9d1c5da8ed",
        "id": "U4oVT04iKrhP"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "    \"PassengerId\": test_df[\"PassengerId\"],\n",
        "    \"Survived\": predictions_test })\n",
        "\n",
        "submission_file = \"submission.csv\"\n",
        "submission.to_csv(submission_file, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "21b294d4940218f5d2bef3782aca9a6ba6e9c054",
        "id": "ppsDjfyoKrhP"
      },
      "source": [
        "3.5 (Optionnel) Exécuter une Cross Validationd ans une boucle pour trouver les meilleures valeurs de learning rate, de keep_prob et de nombre d'epochs.\n",
        "\n",
        "Cross_Validation avec skkearn : https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUdpnq3AKrhP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "xpython",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}