{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxHuvyGEBsgT"
      },
      "source": [
        "# TP SPARK - EPSI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO0f-XvpBsgU"
      },
      "source": [
        "#### Lien utiles\n",
        "\n",
        "Guide Développement :\n",
        "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds\n",
        "\n",
        "Spark Context :\n",
        "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html?highlight=sparkcontext#pyspark.SparkContext\n",
        "\n",
        "Resilient Distributed Dataset (RDD) :\n",
        "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-PIEEBSRBsgU"
      },
      "outputs": [],
      "source": [
        "#!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQDJYEqrBsgV"
      },
      "source": [
        "# Initialisation PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4PurFEWNBsgV",
        "outputId": "e8210efe-e15e-41e4-c076-50b4cd21d103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Conf : \n",
            "    Configuration for a Spark application. Used to set various Spark\n",
            "    parameters as key-value pairs.\n",
            "\n",
            "    Most of the time, you would create a SparkConf object with\n",
            "    ``SparkConf()``, which will load values from `spark.*` Java system\n",
            "    properties as well. In this case, any parameters you set directly on\n",
            "    the :class:`SparkConf` object take priority over system properties.\n",
            "\n",
            "    For unit tests, you can also call ``SparkConf(false)`` to skip\n",
            "    loading external settings and get the same configuration no matter\n",
            "    what the system properties are.\n",
            "\n",
            "    All setter methods in this class support chaining. For example,\n",
            "    you can write ``conf.setMaster(\"local\").setAppName(\"My app\")``.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    loadDefaults : bool\n",
            "        whether to load values from Java system properties (True by default)\n",
            "    _jvm : class:`py4j.java_gateway.JVMView`\n",
            "        internal parameter used to pass a handle to the\n",
            "        Java VM; does not need to be set by users\n",
            "    _jconf : class:`py4j.java_gateway.JavaObject`\n",
            "        Optionally pass in an existing SparkConf handle\n",
            "        to use its parameters\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    Once a SparkConf object is passed to Spark, it is cloned\n",
            "    and can no longer be modified by the user.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from pyspark.conf import SparkConf\n",
            "    >>> from pyspark.context import SparkContext\n",
            "    >>> conf = SparkConf()\n",
            "    >>> conf.setMaster(\"local\").setAppName(\"My app\")\n",
            "    <pyspark.conf.SparkConf object at ...>\n",
            "    >>> conf.get(\"spark.master\")\n",
            "    'local'\n",
            "    >>> conf.get(\"spark.app.name\")\n",
            "    'My app'\n",
            "    >>> sc = SparkContext(conf=conf)\n",
            "    >>> sc.master\n",
            "    'local'\n",
            "    >>> sc.appName\n",
            "    'My app'\n",
            "    >>> sc.sparkHome is None\n",
            "    True\n",
            "\n",
            "    >>> conf = SparkConf(loadDefaults=False)\n",
            "    >>> conf.setSparkHome(\"/path\")\n",
            "    <pyspark.conf.SparkConf object at ...>\n",
            "    >>> conf.get(\"spark.home\")\n",
            "    '/path'\n",
            "    >>> conf.setExecutorEnv(\"VAR1\", \"value1\")\n",
            "    <pyspark.conf.SparkConf object at ...>\n",
            "    >>> conf.setExecutorEnv(pairs = [(\"VAR3\", \"value3\"), (\"VAR4\", \"value4\")])\n",
            "    <pyspark.conf.SparkConf object at ...>\n",
            "    >>> conf.get(\"spark.executorEnv.VAR1\")\n",
            "    'value1'\n",
            "    >>> print(conf.toDebugString())\n",
            "    spark.executorEnv.VAR1=value1\n",
            "    spark.executorEnv.VAR3=value3\n",
            "    spark.executorEnv.VAR4=value4\n",
            "    spark.home=/path\n",
            "    >>> for p in sorted(conf.getAll(), key=lambda p: p[0]):\n",
            "    ...     print(p)\n",
            "    ('spark.executorEnv.VAR1', 'value1')\n",
            "    ('spark.executorEnv.VAR3', 'value3')\n",
            "    ('spark.executorEnv.VAR4', 'value4')\n",
            "    ('spark.home', '/path')\n",
            "    >>> conf._jconf.setExecutorEnv(\"VAR5\", \"value5\")\n",
            "    JavaObject id...\n",
            "    >>> print(conf.toDebugString())\n",
            "    spark.executorEnv.VAR1=value1\n",
            "    spark.executorEnv.VAR3=value3\n",
            "    spark.executorEnv.VAR4=value4\n",
            "    spark.executorEnv.VAR5=value5\n",
            "    spark.home=/path\n",
            "    \n",
            "Spark Context v <property object at 0x7e92501d6340> : \n",
            "    Main entry point for Spark functionality. A SparkContext represents the\n",
            "    connection to a Spark cluster, and can be used to create :class:`RDD` and\n",
            "    broadcast variables on that cluster.\n",
            "\n",
            "    When you create a new SparkContext, at least the master and app name should\n",
            "    be set, either through the named parameters here or through `conf`.\n",
            "\n",
            "    Parameters\n",
            "    ----------\n",
            "    master : str, optional\n",
            "        Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).\n",
            "    appName : str, optional\n",
            "        A name for your job, to display on the cluster web UI.\n",
            "    sparkHome : str, optional\n",
            "        Location where Spark is installed on cluster nodes.\n",
            "    pyFiles : list, optional\n",
            "        Collection of .zip or .py files to send to the cluster\n",
            "        and add to PYTHONPATH.  These can be paths on the local file\n",
            "        system or HDFS, HTTP, HTTPS, or FTP URLs.\n",
            "    environment : dict, optional\n",
            "        A dictionary of environment variables to set on\n",
            "        worker nodes.\n",
            "    batchSize : int, optional, default 0\n",
            "        The number of Python objects represented as a single\n",
            "        Java object. Set 1 to disable batching, 0 to automatically choose\n",
            "        the batch size based on object sizes, or -1 to use an unlimited\n",
            "        batch size\n",
            "    serializer : :class:`Serializer`, optional, default :class:`CPickleSerializer`\n",
            "        The serializer for RDDs.\n",
            "    conf : :class:`SparkConf`, optional\n",
            "        An object setting Spark properties.\n",
            "    gateway : class:`py4j.java_gateway.JavaGateway`,  optional\n",
            "        Use an existing gateway and JVM, otherwise a new JVM\n",
            "        will be instantiated. This is only used internally.\n",
            "    jsc : class:`py4j.java_gateway.JavaObject`, optional\n",
            "        The JavaSparkContext instance. This is only used internally.\n",
            "    profiler_cls : type, optional, default :class:`BasicProfiler`\n",
            "        A class of custom Profiler used to do profiling\n",
            "    udf_profiler_cls : type, optional, default :class:`UDFBasicProfiler`\n",
            "        A class of custom Profiler used to do udf profiling\n",
            "\n",
            "    Notes\n",
            "    -----\n",
            "    Only one :class:`SparkContext` should be active per JVM. You must `stop()`\n",
            "    the active :class:`SparkContext` before creating a new one.\n",
            "\n",
            "    :class:`SparkContext` instance is not supported to share across multiple\n",
            "    processes out of the box, and PySpark does not guarantee multi-processing execution.\n",
            "    Use threads instead for concurrent processing purpose.\n",
            "\n",
            "    Examples\n",
            "    --------\n",
            "    >>> from pyspark.context import SparkContext\n",
            "    >>> sc = SparkContext('local', 'test')\n",
            "    >>> sc2 = SparkContext('local', 'test2') # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            "    Traceback (most recent call last):\n",
            "        ...\n",
            "    ValueError: ...\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "print('Spark Conf :', SparkConf.__doc__)\n",
        "print('Spark Context v %s :'%SparkContext.version, SparkContext.__doc__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VqqLZqKOBsgV",
        "outputId": "4c6ba161-f87b-4913-f69e-2cb410f0622d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "local-1741073154695 3.5.5 \n",
            "---------------\n",
            " [('spark.driver.host', '25a080625eb3'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.name', 'EPSI-TP1'), ('spark.app.id', 'local-1741073154695'), ('spark.app.startTime', '1741073151492'), ('spark.app.submitTime', '1741073151098'), ('spark.executor.id', 'driver'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.port', '45595'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.master', 'local[2]')]\n"
          ]
        }
      ],
      "source": [
        "#local[N] = Pas de cluster Manager. N Nombre de processeur. Si non défini tous les processeurs. Si juste local, un processeur\n",
        "# Une nouvelle exécution nécessite un redémarrage du kernel Jupyter.\n",
        "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"EPSI-TP1\")\n",
        "sc = SparkContext(conf = conf)\n",
        "print(sc.applicationId,sc.version,'\\n---------------\\n',sc.getConf().getAll())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wUa2zySBsgW"
      },
      "source": [
        "## Exercice 1 :  Tableau de nombre."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBdP7uu_BsgW"
      },
      "source": [
        "#### 1.1 Créer un RDD avec 1000 chiffres. Afficher 5 exemples pris au hasard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CbCXaJ9LBsgW",
        "outputId": "6b799268-5d67-4da3-faa4-756f1abebcdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 880, 566, 161, 856]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "rdd = sc.parallelize(range(1000)) #RDD = Données Distribuées Résilientes. Stockage des chiffres de 0 à 999 avec range.\n",
        "rdd.takeSample(False, 5) # 5 valeurs au hasard. False car sans remplacement."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eiJNwjjjDb4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue3XhFDdBsgW"
      },
      "source": [
        "## Exercice 2 : WordCount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4voL1ntBsgW"
      },
      "source": [
        "#### 2.1 Charger le fichier data/montaigne.txt ligne par ligne. Afficher 5 examples pris au hasard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kpAVhppBsgW"
      },
      "outputs": [],
      "source": [
        "text_file = sc.textFile(\"/content/drive/MyDrive/Colab Notebooks/Montaigne.txt\")\n",
        "text_file.takeSample(False, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB-oaUntBsgW"
      },
      "source": [
        "#### 2.2 Segmenter chaque ligne en mots. Afficher 2 exemples au hasard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFktustLBsgX"
      },
      "outputs": [],
      "source": [
        "words = text_file.flatMap(lambda line: line.split())\n",
        "words.takeSample(False, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKWj9_O3BsgX"
      },
      "source": [
        "#### 2.3 Segmenter chaque ligne en mots en rendant chaque mot indépendant. Afficher 5 exemples au hasard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aOEw6I9BsgX"
      },
      "outputs": [],
      "source": [
        "words = text_file.flatMap(lambda line: line.split())\n",
        "words.takeSample(False, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QcCuyOdBsgX"
      },
      "source": [
        "#### 2.4 Afficher le nombre total de mot, et la taille du vocabulaire (nombre de mots distinct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "252pkHDfBsgX"
      },
      "outputs": [],
      "source": [
        "total_words = words.count()\n",
        "vocabulary_size = words.distinct().count()\n",
        "\n",
        "print(f\"Total number of words: {total_words}\")\n",
        "print(f\"Vocabulary size (number of distinct words): {vocabulary_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU4x8Ar4BsgX"
      },
      "source": [
        "#### 2.5 Transformer chaque mot w en une clé-valeur (w,1). Afficher 5 exemples de clé-valeur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cruqvgi1BsgX"
      },
      "outputs": [],
      "source": [
        "word_counts = words.map(lambda word: (word, 1))\n",
        "word_counts.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DajcYPN8BsgX"
      },
      "source": [
        "#### 2.6 Sommer  par clé . Afficher 5 valeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl96J23fBsgX"
      },
      "outputs": [],
      "source": [
        "word_counts = word_counts.reduceByKey(lambda a, b: a + b)\n",
        "word_counts.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gE4pA_2BsgX"
      },
      "source": [
        "#### 2.7 Ne conserver que les mots de plus de 6 caractères apparaissant au moins 3 fois"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiH_M0IFBsgX"
      },
      "outputs": [],
      "source": [
        "filtered_word_counts = word_counts.filter(lambda x: len(x[0]) > 6 and x[1] >= 3)\n",
        "filtered_word_counts.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QbqMXizBsgX"
      },
      "source": [
        "####  2.8 Récupérer le résultat des actions précédentes. Afficher le type de la valeur renvoyée"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8CuMvVbBsgY"
      },
      "outputs": [],
      "source": [
        "result = filtered_word_counts.take(10)\n",
        "\n",
        "print(\"Result:\", result)\n",
        "print(\"Type of result:\", type(result))\n",
        "\n",
        "result2 = word_counts.take(5)\n",
        "print(\"Result:\", result2)\n",
        "print(\"Type of result:\", type(result2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vy-2NKUBsgY"
      },
      "source": [
        "#### 2.9 Trier et afficher les mots trouvés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wj_LrR-mBsgY"
      },
      "outputs": [],
      "source": [
        "sorted_word_counts = filtered_word_counts.sortBy(lambda x: (-x[1], x[0]))\n",
        "\n",
        "sorted_results = sorted_word_counts.collect()\n",
        "\n",
        "for word, count in sorted_results:\n",
        "    print(f\"{word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyOTDuuRBsgY"
      },
      "source": [
        "## Exercice 3 optionnel : Analyse numériques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAZnpZHdBsgY"
      },
      "source": [
        "#### 3.1 Charger dans Spark le fichier Titanic.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L646BAmeBsgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRvE9_dzBsgY"
      },
      "source": [
        "#### 3.2 Creer une fonction python qui transforme une ligne en une liste de variables avec le type correct. tester sur une ligne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IErMZsk2BsgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDn-T8CPBsgY"
      },
      "source": [
        "#### 3.3 Charger les segmentations dans un RDD. Afficher les trois premiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X3qzpPnBsgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdGG_q-JBsgY"
      },
      "source": [
        "#### 3.4 Creér un RDD composé de la classe (élément 2)  et de la survie (élement 1). Afficher les 10 premiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEBty4fnBsgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyqBZR8HBsgY"
      },
      "source": [
        "#### 3.5 En utilisant la fonction mapValues, créer une liste avec la classe du passager et un doublet (survie,1). Afficher les 10 premiers.\n",
        "Exemple pour les deux premiers passagers : <br>\n",
        "(3, (0, 1)), <br>\n",
        "(1, (1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehUcy9ISBsgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBnDAvHxBsgg"
      },
      "source": [
        "#### 3.6. Pour chaque classe afficher le nombre de survivant et le nombre total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLzgkrT4Bsgg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqhICYF_Bsgg"
      },
      "source": [
        "#### 3.7 afficher le taux de survie par Classe, trié de 1 à 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XctN4AeQBsgg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmIASomLBsgg"
      },
      "source": [
        "#### 3.8 Créer un rdd des survivants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AELZ-IIBsgg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq-t3UMGBsgh"
      },
      "source": [
        "#### 3.9 : Trouver le survivant le plus agé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW8RJ0roBsgh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "f7976576504ac6c456dadd405d7477574ca2a64265ee4724cfbc25daae5f6d94"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}